"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"prompt-architect-complex-content","metadata":{"permalink":"/blog/prompt-architect-complex-content","source":"@site/blog/2025-11-12-prompt-architect-complex-content.mdx","title":"Prompt architecting complex content","description":"A few weeks ago in October, I completed two courses back-to-back, in prompt engineering and information architecture. The prompt engineering course taught me about using AI as an outline builder. The IA course gave me a comprehensive view of taxonomy, content modeling, navigation design, all the structured thinking that makes information findable and usable.","date":"2025-11-12T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"prompt-architect-complex-content","title":"Prompt architecting complex content"},"unlisted":false,"nextItem":{"title":"Iterative auditing as a progress tracking mechanism","permalink":"/blog/analysis-paralysis"}},"content":"A few weeks ago in October, I completed two courses back-to-back, in prompt engineering and information architecture. The prompt engineering course taught me about using AI as an outline builder. The IA course gave me a comprehensive view of taxonomy, content modeling, navigation design, all the structured thinking that makes information findable and usable.\\n\\nAnd I couldn\'t help but wonder: *Could I use prompt engineering to build a course about prompt engineering for information architects?*\\n\\nIt wasn\'t just about creating a course. It was about testing my new skills and AI. Could I use my IA expertise to validate AI-generated content at scale? Could I set up guardrails that would maintain content credibility? How far could Claude and I get while staying truthful?\\n\\nI decided to find out!\\n\x3c!--truncate --\x3e\\n### The collaborative discovery\\n\\nI didn\'t start with \\"Here\'s my outline, fill it out.\\" I started with a question-asking prompt:\\n\\n```\\nAct as an expert information architect with expert prompt engineering skills. \\nHelp me design a course on prompt engineering for budding information architects. \\nAsk me questions until you have enough data to develop this course.\\n\\nAsk me the first question now.\\n```\\n\\nClaude then asked me ten detailed questions. Far from surface-level, these were deep questions about target audiences, learning outcomes, technical approach, pain points to address, pedagogical style...questions that made me think harder about what I was actually trying to build.\\n\\nWhat resulted was a comprehensive course design summary that transformed my idea into something truly substantial:\\n\\n- 16 hours of content across 4 weeks\\n- Four major sections: Foundations, Core IA Tasks, Advanced Applications, Tool Building\\n- Self-paced, practical, focused on real-world deliverables\\n- No-code friendly but with optional prototyping\\n\\n### The meta-exercise structure\\n\\nThis experiment fascinates me because it operates on three interconnected levels simultaneously. \\n- **Testing my IA knowledge** by having me validate AI-generated IA concepts\\n- **Testing my prompt engineering skills** in building complex content systematically\\n- **Testing Claude\'s capabilities** with strict \\"no hallucination\\" directives\\n\\n\\n```mermaid\\ngraph LR\\nIA[IA Expertise] --\x3e|validates| Content[Course Content]\\nContent --\x3e|demonstrates| PE[Prompt Engineering]\\nPE --\x3e|tests| IA\\nPE --\x3e|generates| Content\\nContent --\x3e Credibility{Credibility Check}\\nCredibility --\x3e|human audit| Truth[Truthful?]\\nCredibility --\x3e|skills audit| Next[Next Blog Post]\\n\\nstyle Content fill:#e1f5ff\\nstyle Credibility fill:#fff4e1\\nstyle Truth fill:#f0f0f0\\n\\n```\\n\\n### Three days of generation\\n\\nOnce I had the outline, I switched to a template-driven approach (another lesson from my prompt engineering course):\\n\\n```\\nAct as an outline expander information architect. Create a new outline \\nfor each bullet point in the content I give you. Then, develop detailed \\ncontent for the outline. At the end, ask me for the next set of content \\nto develop.\\n\\nStart with Module n.n. Outline each bullet point and develop detailed \\ncontent for them now.\\n```\\n\\nThis prompt became my workhorse. Module after module, Claude would:\\n\\n1. Take my content bullets\\n2. Expand them into detailed outlines\\n3. Flesh out the outlines with examples, exercises, code samples\\n4. Ask for the next module\\n\\nAt the end of three days, I had 11 detailed modules with learning objectives, exercises, self-assessment projects, prompt pattern libraries, and code examples.\\n\\nThe consistency was remarkable. Despite being built in chunks over three days, the content flows seamlessly.\\n\\n### The credibility paradox\\n\\nLet\'s get honest.\\n\\n**What exceeded my expectations:**\\n\\n- The depth and detail of generated content\\n- The consistency across 11 modules using template prompts\\n- The complexity of code examples that Claude produced\\n- How well the high-level concepts aligned with actual IA principles\\n\\n**What I\'m still validating**: Every. Single. Code. Example.\\n- Whether the exercises actually work as designed\\n- If the prompt patterns produce the claimed results\\n- Whether the examples are truly original (not memorized content)\\n\\nThis is the most sobering part of the experiment: **generation speed does NOT equal validation speed.**\\n\\nI built the course in 3 days. I\'ve been validating and converting it to Mintlify for over a week now, and I\'m not done.\\n\\nModule 2.2 (Content Modeling) required three runs of my template prompt because I kept hitting context window limits. I\'ve learned the hard way to break large tasks into mini-tasks that AI can handle comfortably.\\n\\nThe Taxonomy module (Module 2.1) is my favorite so far, because I actually learned from it while validating it. The content wasn\'t plagiarized, and wasn\'t based on existing course outlines. It was genuinely useful!\\n\\n### Audit in progress\\n\\nThis course exists in an interesting liminal space:\\n\\n**What it IS:**\\n\\n- A demonstration of what AI can do with expert guidance\\n- A prompt engineering portfolio piece\\n- An experiment in credibility guardrails\\n- A learning tool for me about both IA and prompt engineering\\n\\n**What it is NOT (yet):**\\n\\n- Ready to teach to others\\n- Fully validated and tested\\n- A finished product\\n\\nI\'m now tweaking the existing Claude Skills to help audit this course.\\n\\n### What I\'ve learned about prompts\\n\\n**Prompts are powerful, powerful tools.** They can make or break your AI experience.\\n\\nThe evolution of my prompting strategy sums it up:\\n\\n1. **Discovery phase**: \\"Ask me questions\\" (Collaborative, exploratory)\\n2. **Structure phase**: \\"Generate an outline\\" (Architectural, high-level)\\n3. **Expansion phase**: \\"Expand each bullet point\\" (Systematic, detailed)\\n\\nEach phase required a different prompt architecture. The template-driven approach in phase 3 was crucial for maintaining consistency across 11 modules.\\n\\nAnd my most critical insight: **Fast generation requires slow validation.**\\n\\nAI creating content \\"in seconds\\" is just the generation phase. Verifying accuracy, testing examples, checking for hallucinations, ensuring originality, and validating against domain expertise, that\'s where human expertise and knowledge become essential.\\n\\n### Was this a successful experiment?\\n\\n***Absolutely.***\\n\\nThis is a prime example of \\"Wow, look what AI can do with expert guidance!\\"\\n\\nThe course demonstrates:\\n\\n- How structured prompting can generate complex, consistent content\\n- How domain expertise creates essential validation layers\\n- How AI can be a genuine learning partner\\n- How important it is to break large tasks into manageable chunks\\n- The critical difference between generation and validation\\n\\n*I\'m not done though.* This experiment continues through the audit phase, where I\'m using AI tools to audit AI-generated content about AI tools for information architects. Stay tuned for my next blog post!"},{"id":"analysis-paralysis","metadata":{"permalink":"/blog/analysis-paralysis","source":"@site/blog/2025-11-11-analysis-paralysis.mdx","title":"Iterative auditing as a progress tracking mechanism","description":"The UX audit returned 18 findings on my Ikigai app (including a critical one that called the app \\"boring\\"!)","date":"2025-11-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"analysis-paralysis","title":"Iterative auditing as a progress tracking mechanism"},"unlisted":false,"prevItem":{"title":"Prompt architecting complex content","permalink":"/blog/prompt-architect-complex-content"},"nextItem":{"title":"Watching Claude build an audit system using Notion MCP","permalink":"/blog/notion-mcp-audits"}},"content":"The UX audit returned 18 findings on my Ikigai app (including a critical one that called the app \\"boring\\"!) \\n\\nInstead of diving into fix-mode right away, I chose to run the remaining audit skills I had built, thinking a complete picture would help before I start making any changes.\\n\\nLooking back, this was both a good and a bad decision.\\n\x3c!-- truncate --\x3e\\n\\n### The Marie Kondo approach to technical debt\\n\\nA year ago, I *Marie Kondo\u2019d* my wardrobe and cut my belongings in half. The method worked for me because it made me analyze every single item I owned before deciding what to keep. Realizing the scope clearly is how I decided what actually mattered.\\n\\nI wanted that same clarity with my audit findings. I planned to run every skill, collect every issue, and comprehend the scope before trying to fix anything, maybe even detect overlapping issues, if any. I made sure to document everything in Notion databases so nothing would get lost.\\n\\nSo Claude and I ran the accessibility audit next, then security, then content accuracy. No code review audit as we had generated enough data to work with now!\\n\\nClaude Skills work like a charm. Claude had designed the Notion databases along with the skills, and seeing the findings directly filed into separate databases felt like having an expert team review my beginner code. Each audit filled its own database with severity ratings, specific recommendations, and links back to the audit session summaries.\\n\\nThen I looked at the numbers.\\n```\\nTotal Issues: 87\\n\\n\u251c\u2500 Content Accuracy: 32 findings\\n\u251c\u2500 Accessibility: 24 findings  \\n\u251c\u2500 UX/UI: 18 findings (1 critical)\\n\u2514\u2500 Security: 13 findings\\n```\\n***Yikes.***\\n\\n### The security wake-up call\\n\\nThe security audit surfaced something I\'d been completely oblivious to while building: \\n\\n> Cross-Site Scripting (XSS) vulnerabilities through unsanitized user input rendered via innerHTML, combined with localStorage data exposure risk across browser users.\\n\\nSeems obvious now, but as a non-coder building my first web app, I had no idea I was creating security holes. I\'d been focused on making the interface aesthetically pleasing while missing fundamental safety issues.\\n\\nI\'ve since added a privacy warning to the app, but the finding exposed how much I didn\'t know about what I didn\'t know.\\n\\n### The paradox of good data\\n\\nThe problem with systematic AI audits is that they work exactly as designed.\\n\\n87 documented issues, each with severity ratings and specific recommendations, organized across multiple Notion databases. No ambiguity about what needs fixing. No wondering if I missed something important. Just an overwhelming number of open issues staring back at me.\\n\\nThe same completeness that makes this data valuable makes it paralyzing. Where do you even start when you have 32 content accuracy findings alone?!\\n\\n### Decisions, decisions\\n\\n```\\nShould I start fixing issues immediately?\\n\\n\u2502\\n\u251c\u2500 YES \u2192 Risk missing patterns across audits\\n\u2502        Risk fixes breaking other things\\n\u2502        Risk losing track of what changed\\n\u2502        No version control = no progress history\\n\u2502\\n\u2514\u2500 NO \u2192 See the full scope first\\n        Understand overlaps (UX + Security?)\\n        Plan a methodical approach\\n        Set up proper tracking before touching code\\n\\nDecision: Run all audits first, don\'t fix issues immediately\\n\\nResult: 87 issues documented\\n\\nCurrent Status: Still figuring out how much of the fix approach to automate\\n\\n```\\n\\n### What I\'m learning about prioritization\\n\\nIt\'s easy to miss the forest for the trees. My technical writing background helps here, though. In documentation, we\'re often the chosen ones who understand what every part of a system does and how pieces connect. We can\'t fixate on perfecting one section while ignoring an overall structure that doesn\'t make sense.\\n\\nMy plan is to maintain a working app at all times, even if it\'s not the most polished version. Progress over perfection means the app remains functional through iterations, not beautiful, but certainly not broken. \\n\\n> ***Perfect** is the enemy of **Done**!* \\n\\n### The current state\\n\\nThose 87 issues in Notion, unfixed, are both motivating and pressure-inducing. I have to remind myself that this is a personal project meant to be fun, that I\'m proving AI efficiency to myself, not shipping enterprise software. But the high number of findings does feel like a real-world scenario, and I want to approach this methodically, not just dive in randomly.\\n\\nCurrent rumination: finalize a logical system for walking through fixes in order of actual priority, not in order of whatever catches my eye first. Start collaborating with Claude Code...\\n\\n### GitHub comes first\\n\\nBefore touching any code, I need version control in place. The app currently exists as a local file on my computer that I keep improving. It\'s way too easy to lose track of what changed and why.\\n\\nThis project is specifically about demonstrating how AI helps track improvements through connected tools, like Skills for automated auditing, Notion for issue documentation, and GitHub for code versioning. I want to showcase this progression and measure whether subsequent audits demonstrate actual improvement.\\n\\n### The *real* discovery\\n\\nI built these audit skills thinking they\'d be one-time debugging tools. Run the audit, get the findings, fix the issues, done.\\n\\nBut documenting 87 issues systematically revealed something more valuable: this isn\'t a debugging tool, it\'s a **progress tracking system**.\\n\\nWhen I run the same audits next week, next month, whenever, I\'ll have objective proof of whether things improved, stagnated, or regressed. No guessing whether changes made things better; I\u2019ll have data showing what shifted and by how much.\\n\\nThe overwhelm comes from seeing everything at once. The value comes from being able to measure movement over time.\\n\\n### What\'s next\\n\\n1. Get the Ikigai app into GitHub with proper commit structure. Every fix should be a documented commit so there\'s a clear history of what changed.\\n2. Fix a small batch of issues manually to understand the patterns. Maybe start with the critical security findings, then see what other issues naturally resolve as side effects.\\n3. Run the audits again. Compare findings. See if the Claude skills can identify improvements, not just problems.\\n\\nThe goal isn\'t to get to zero issues immediately. It is to establish a workflow where progress is visible, measurable, and documented, and where AI helps **implement and track solutions**."},{"id":"notion-mcp-audits","metadata":{"permalink":"/blog/notion-mcp-audits","source":"@site/blog/2025-11-08-notion-mcp-audits.mdx","title":"Watching Claude build an audit system using Notion MCP","description":"I had built five audit skills for my Ikigai app - UX, Code Quality, Content Accuracy, Accessibility, and Security - where each one could analyze the app and generate unique, detailed findings as expert team personas. But the audit reports existed only as chat artifacts, and I wanted them in Notion where I could track fixes, link to GitHub issues, and measure progress over time.","date":"2025-11-08T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"notion-mcp-audits","title":"Watching Claude build an audit system using Notion MCP"},"unlisted":false,"prevItem":{"title":"Iterative auditing as a progress tracking mechanism","permalink":"/blog/analysis-paralysis"},"nextItem":{"title":"Building an app from a PDF (because AI can!)","permalink":"/blog/diving-into-ai"}},"content":"I had built five audit skills for my Ikigai app - UX, Code Quality, Content Accuracy, Accessibility, and Security - where each one could analyze the app and generate unique, detailed findings as expert team personas. But the audit reports existed only as chat artifacts, and I wanted them in Notion where I could track fixes, link to GitHub issues, and measure progress over time.\\n\\nOn prompting, Claude suggested the database schemas I\'d need in Notion to track audits: ten databases - five for audit sessions, five for individual findings - all with proper schemas and relations between them. \\n\\nI manually created one database in Notion to test the concept. Getting every property right - the select options, the number fields, the relations - took focus that would now need to last for ten databases.\\n\\nSo instead, I set up the Notion MCP connection and asked Claude to complete the task.\\n\x3c!--truncate --\x3e\\n\\n### The setup\\n\\nI followed <a href=\\"https://developers.notion.com/docs/mcp\\" target=\\"_blank\\">Notion\'s official MCP documentation</a>. After initial hiccups caused by Claude\'s integration docs, I created a Notion MCP -> Claude integration, got the API key, and added it to Claude Desktop\'s config file. The authentication part was straightforward.\\n\\nFair warning though, this integration needs refreshing everyday (or maybe that\'s just Claude being moody?); Claude refuses to sync with Notion when I try to pick up database tasks from where I left off the previous evening. I always rejig the config now before starting a database sync.\\n\\nNext, I directed Claude to write a Node.js setup script with checkpoint functionality in case anything failed midway. I needed it to create all ten databases with complete schemas, then link them together with relations. I can\'t, and didn\'t have to, write a single line of the setup code!\\n\\n### The moment!\\n\\nI ran the command, and Claude started creating the databases. I switched to Notion and hit Refresh. Ten databases appeared! Each one properly configured: UX Audit Sessions with all its metrics, UX Findings with severity levels and status tracking, Code Quality Sessions, Code Quality Findings, everything. \\n\\nIt warranted a little dance!\\n\\n### Running the first audit\\nThe app\'s first draft had obvious UX issues that jumped out at me immediately, and I wondered if the UX Auditor Skill would catch the same annoyances.\\n\\nI typed: \\n> Use the UX Auditor skill. File: xyz.html. After the audit, save results to Notion.\\n\\nTBH, I half-expected something to break, or have Claude tell me there\'s a property mismatch or a missing field, or that some piece of code wasn\'t working. \\n\\nClaude ran the audit and found 18 UX issues including one critical finding about the app being boring(!). The UX Auditor caught every issue that was bugging me at first glance, and then some. The audit was more nuanced than I expected, with detailed severity ratings and specific recommendations.\\n\\nClaude then searched for the relevant Notion databases, created an audit session entry, created 18 individual finding entries, linked them all together, and gave me the Notion URLs.\\n\\nJust like that.\\n\\n### The two-database pattern\\n\\nI hadn\'t immediately understood why Claude created two databases per audit type initially, but it made sense as soon as I looked at the audit data.\\n\\nFor each skill:\\n* One database holds the session, the macro view: date, file analyzed, total findings count, severity distribution, overall scores. The executive summary, if you will.\\n\\n* The other holds individual findings, the details. Each specific issue with its location, current state, recommendation, effort estimate, and status.\\n\\nSessions link to findings. Findings link back to sessions. You can view either the forest or the trees.\\n\\n### The (technical) reality check\\n\\nI ran the Content Accuracy Checker skill to see if I\u2019d just gotten lucky with the first audit or if this was a working idea. It was, to an extent, beginner\u2019s luck. While the audit ran great and Claude managed to publish everything to Notion, we hit a technical issue toward the end, and the chat timed out. My Claude chat history has no record of those Content Accuracy findings! \\n\\nThis made me think about all the previous instances where I\'ve lost entire chats because Claude timed out or hit context limits. Maybe writing Claude outputs to a linked external database should be the norm for any work?\\n\\n### What\'s next\\n\\nThe integration works well. I have two audits completed with reports in Notion, ready to track. I want to turn Notion findings into GitHub issues now to track completion and link to release notes, etc.\\n\\nThe audit skills themselves could become measurement tools. If I keep the check parameters unchanged, running audits at regular intervals could give me an objective progress marker. \\n```\\nFix issues --\x3e run audit again --\x3e compare results = hard quality numbers!\\n```\\n\\n### The learning\\nI now know to keep my requests to Claude simple. The clearer and more direct the task, the better the results. No need to overthink the prompts.\\n\\nThis wasn\'t about me writing clever code or complex prompts. It was about Claude understanding the schemas needed and building them correctly. The manual work I\'d done on that first database taught me what properties mattered. But scaling that to ten databases with relations, the setup script, the database schemas, the Notion integration, the audit execution was all Claude.\\n\\n(MCP FTW!)\\n\\nHere are snapshots of the Notion audits:\\n\\n![ux-audit](/img/ux-audit.png)\\n***\\n\\n![accessibility-audit](/img/accessibility-audit.png)\\n***\\n\\n![security-audit](/img/security-audit.png)\\n***\\n\\n![content audit](/img/content-audit.png)"},{"id":"diving-into-ai","metadata":{"permalink":"/blog/diving-into-ai","source":"@site/blog/2025-10-01-diving-into-ai.mdx","title":"Building an app from a PDF (because AI can!)","description":"AI and I built an interactive web app from a PDF, with drag-and-drop, auto-save, and an interactive five-step workflow, in a week! (I\'m thrilled about how well it works too!)","date":"2025-10-01T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"diving-into-ai","title":"Building an app from a PDF (because AI can!)"},"unlisted":false,"prevItem":{"title":"Watching Claude build an audit system using Notion MCP","permalink":"/blog/notion-mcp-audits"},"nextItem":{"title":"Sabbaticals - Yea or Nay?","permalink":"/blog/sabbaticals"}},"content":"AI and I built an interactive web app from a PDF, with drag-and-drop, auto-save, and an interactive five-step workflow, in a week! (I\'m thrilled about how well it works too!)\\n\\nThis project was about converting a PDF (and complex philosophical concepts) into intuitive user experiences while leveraging GitHub Copilot and Claude for rapid prototyping and problem-solving. \\n\\nI\'m documenting what I\'ve learned about collaborating with AI to build something real.\\n\x3c!-- truncate --\x3e\\n### Why Ikigai\\nI first learnt about \u201cIkigai\u201d in the book *\'Ikigai: The Japanese Secret to a Long and Happy Life\' by Francesc Miralles and Hector Garcia*. Although I later discovered that the book, as well as the Positive Psychology PDF that this app is is based on, are nothing like the actual Japanese concept; they\u2019re more a cultural appropriation of the eastern philosophy. \\n![Ikigai venn diagram](/img/ikigai.png)\\n\\nThat said, I find the framework insightful. The guided process of exploring passion, mission, vocation, and profession resonates with me, and I do think people could benefit from it as an interactive tool.\\n\\n### The fear\\nMy biggest concern isn\u2019t failure, it\'s not knowing how to handle AI-generated code if something goes wrong. If the code breaks or becomes messy, I wouldn\'t know where to begin fixing it, as a non-coder. I\'ve learned though, that the trick to working with AI is to build piece-by-piece, test frequently, and catch problems early.\\n\\n### First prompt\\nI started with:\\n\\n> ***I am building an Ikigai web-app based on the Ikigai PDF. Analyze the attached files and list the top improvements you would make to have a fully-functional, highly modern, well-designed interactive app that helps the reader find their Ikigai. \\n> <br/><br/>The PDF is the only source of information. \\n> Do not hallucinate new content about Ikigai, and do not create new artifacts unless asked to. \\n> Do not skim over or lose the details in the PDF. \\n> <br/><br/>Ask me questions on the intended behaviour, design, purpose, etc of the app until you have enough information for your research and analysis.***\\n\\n\\nThis prompt set the tone for everything. I established boundaries, provided constraints, and requested collaboration. It was a framework for how I wanted to work with AI.\\n\\n### What clicked\\nPrompt engineering felt abstract until I started practicing it. Prompts that clicked:\\n- **Few-shot prompts**: Showing examples helped Claude understand my expectations better than lengthy explanations.\\n- **Chain of thought prompts**: Asking Claude to think through problems step-by-step before generating code reduced errors.\\n- **Outline expansion prompts**: Starting with a simple structure and expanding section by section kept things manageable.\\n- **Flipped interaction prompts**: Instead of telling Claude what to do, I\'d ask it to interview me about my goals. This made the collaboration feel more like partnership.\\n\\n### What I like about the app\\n- Step 2: Guides users through questions about what they love, what they\'re good at, what the world needs, and what they can be paid for. The UI provides thoughtful prompts and self-reflective questions for each circle.\\n- Step 3: Find overlapping responses, where users identify which responses fit into multiple circles, and overlaps reveal patterns.\\n\\nThese steps feel deeply insightful. They\'re also the pages I plan to make more \u201cmine\u201d as the project evolves.\\n\\n### What\'s next\\nNow that Phase 1 is complete, here\'s the plan for Phase 2:\\n1. Run the audit skills\\n   \\n   Claude and I built 5 *Claude Skills* to audit the app:\\n   - UX/UI audit\\n   - Code quality review\\n   - Content accuracy check\\n   - Accessibility audit\\n   - Security analysis\\n\\n   I\u2019ve set up Notion to track the issues that these Skills generate. Claude designed the (ten) databases required to store these issues by Skill type. All I had to do was nudge it in the direction I wanted. (*More about this in the next blog!*)\\n\\n2. Fix issues\\n   - Set up an issue tracker via GitHub MCP with Notion\\n   - Find a way to automate issue fixes (ambitious, I know)\\n\\n### Frustrations and challenges\\nClaude routinely generated code for alternate files without me asking for them. Sometimes even after I specifically told it not to, it would create new artifacts when all I needed was iterations on what we had.\\n\\nI learned to use Claude Projects and project descriptions to set persistent context, and used a universal \'Truth Protocol\' to remind Claude of my working style. I even reset the chat a few times to get back on track. This wasn\'t Claude being bad; it was me learning how to communicate better.\\n\\nI couldn\'t get the Notion MCP working initially because of conflicting setup info from both Claude and Notion - the documentation didn\'t align and I hit a wall. I have a similar challenge ahead: integrating with GitHub MCP to make app updates trackable through versions.\\n\\nI\'m also about to run audit skills on the app for UX, accessibility, security, code quality, and content accuracy, and the resulting number of issues might get overwhelming.\\n\\n### What I\'ve learned\\nI\'m excited at having gotten this far! With AI, I just have to think an idea through and choose my prompts.\\nMy top takeaways:\\n1. AI slop is real. A human in the loop is non-negotiable. AI is a tool, not the solution.\\n2. Start with constraints. Tell the AI what not to do as clearly as what to do.\\n3. Collaborate, don\'t dictate. Ask AI to interview you about your goals before generating solutions.\\n4. Test frequently. The worst-case scenario isn\'t building something broken. It\'s building something you can\'t debug.\\n5. Use Projects to set persistent context. It saves you from repeating yourself.\\n6. Expect frustration; AI will misunderstand you, and you\'ll reset chats. That\'s part of the process!\\n7. Training in prompt engineering helped me a lot. I highly recommend the entire series by Prof Jules White, Vanderbilt University on Coursera.\\n\\n### Why this matters\\nI enjoy research and problem solving. I haven\'t pinpointed my mission or even my *ikigai* yet, but I do enjoy the learning that came with building this app.\\nAnd maybe that\'s just it!"},{"id":"sabbaticals","metadata":{"permalink":"/blog/sabbaticals","source":"@site/blog/2020-12-30-sabbaticals.mdx","title":"Sabbaticals - Yea or Nay?","description":"YEA!","date":"2020-12-30T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"sabbaticals","title":"Sabbaticals - Yea or Nay?"},"unlisted":false,"prevItem":{"title":"Building an app from a PDF (because AI can!)","permalink":"/blog/diving-into-ai"},"nextItem":{"title":"Design Thinking for solo technical writers","permalink":"/blog/design-thinking"}},"content":"**YEA!**\\n\\nAfter 12 years of tech writing in the corporate world, I went on a sabbatical in 2016. I had no plan or alternate source of income. (Must say, a little too adventurous even for me!) \\n\x3c!-- truncate --\x3e\\nI was driven to do something, *anything*, radically different from my routine.\\n\\nIn the months that followed, I immersed myself in online courses ranging from UX to entrepreneurship. Whatever caught my fancy, really. Spent my free time volunteering at <a href=\\"https://charlies-care.com/\\" target=\\"/_blank\\">my favorite animal shelter</a>, traveling, and clicking pics of <a href=\\"https://www.flickr.com/photos/sabitarao/48670537817/\\" target=\\"_blank\\">my muse</a>. I completed my AOW scuba certification too, and seriously contemplated a career change!  \\n\\n![scuba diving](/img/scuba.jpeg)\\n*(Clicked at 30m/100ft on the Great Barrier Reef seafloor. I\'m in the middle, with the funny hair.)*  \\n\\nIn early 2018, I dived into courses again, this time with a purpose. I focused on <a href=\\"https://www.freecodecamp.org/sabitarao\\" target=\\"_blank\\">front-end web development</a> and Docs as Code. Started giving corporate interviews to see where I stood in the tech writing world that I\'d left behind.\\n\\nInterviews can whip you into shape like nothing else! The feedback and advice I received from interviewers helped me dig deep, acknowledge my weaknesses, and flaunt my strengths. \\n\\n2019 saw me apply to Google\'s Season of Docs and not make the cut. That was rough, but helped me realize the caliber of writers I was up against. So I improved my skills, participated in Hacktoberfest, and landed a role in a startup. For a geek like me, being immersed in a barrage of new tech, surrounded by brilliant minds, and practically getting paid to learn, is *gold*. \\n\\nNone of this (okay, maybe *some*) would\'ve happened without a sabbatical. It made me *want* to be a technical writer again. That little step away packed a ton of perspective!  \\n\\n(As for scuba diving, well, it\'s not off the table. :-D ) \\n\\nIf a clean break from work isn\'t your thing, that\'s fine too. Find other ways to infuse your routine with new energy. Volunteer, plan weekend getaways, learn a new skill at work, participate in hackathons, the list is endless. The keyword here is *balance*. Don\'t bite off more than you can chew, and don\'t compare yourself to others.\\n\\nP.S. I ended 2020 as a contributor to CERN-DESY in Google\'s Season of Docs! :) More <a href = \\"https://github.com/sabitarao/gsod/wiki\\">here</a>."},{"id":"design-thinking","metadata":{"permalink":"/blog/design-thinking","source":"@site/blog/2020-04-11-design-thinking.mdx","title":"Design Thinking for solo technical writers","description":"What does design thinking have to do with writing, or any task, really?","date":"2020-04-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"design-thinking","title":"Design Thinking for solo technical writers"},"unlisted":false,"prevItem":{"title":"Sabbaticals - Yea or Nay?","permalink":"/blog/sabbaticals"}},"content":"What does design thinking have to do with writing, or any task, really?\\n\x3c!-- truncate --\x3e\\n<p>![design thinking work flow](/img/design-thinking.jpg) Image credits: <a href=\\"https://www.flickr.com/photos/christineprefontaine/8667743577\\" target=\\"_blank\\">Christine Prefontaine</a></p>   \\n\\nI\'m an IBM-certified <a href=\\"https://www.credly.com/badges/573fca7e-9b8e-4a7b-a50e-13088787002f/public_url\\" target=\\"_blank\\">Design Thinking Co-Creator</a>. It\'s serendipity! I stumbled upon IBM\'s <a href=\\"https://www.ibm.com/design/thinking/page/courses/Practitioner\\" target=\\"_blank\\">series</a> a year ago, while googling unrelated topics.  \\n\\nAs a technical writer, it\'s my job to focus on user goals in all my tasks. And as a remote worker, I sometimes run the risk of missing outliers while planning doc structures, followed by writing sprints, reviews, improvs...you know the drill.  \\n\\nIt\'s hard then, to not dread stakeholders\' meetings that could result in drastic edits, threatening those perfectly-crafted sentences and info architecture layouts.\\n\\nAnd that\'s exactly the sort of risk that design thinking can *minimize*. With a specific set of tools and processes, it requires team members across job roles to truly **collaborate**, keeping end-user goals at the center of discussions. Everyone in the team stands to gain.  \\nFor example, your perspective on the product UI or workflows can improve the end user\'s experience. Similarly, giving dev teams a say in doc design/planning/publishing makes them care about product docs. True story!  \\n\\n![restless reinvention](/img/restless-reinvention.PNG)  \\nDesign thinking reminds me to focus on empathy, human-centered solutions, and continuous improvement.  \\nIf I have to pick one aspect of design thinking that (sole) writers must imbibe, it\'s this:  \\n*Fail fast and cheap*.  \\nBy *not* aiming for a \'perfect\' draft, I spend less time theorizing about possible solutions, and more time actually improving ones that work.  \\n\\n>**The only constant in life is change.** \\n>~Heraclitus"}]}}')}}]);