"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"content-ontology-pivot","metadata":{"permalink":"/blog/content-ontology-pivot","source":"@site/blog/2025-11-22-content-ontology-portfolio-case-study.md","title":"Unstructured content, the bane of RAG systems. Or is it?","description":"When I started my Prompt Engineering for IAs course generation experiment, it was meant to be a simple exercise to understand how context windows work. Somewhere along the way though, I found myself staring at 20000+ lines of unstructured text across 11 modules.","date":"2025-11-22T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"content-ontology-pivot","title":"Unstructured content, the bane of RAG systems. Or is it?"},"unlisted":false,"nextItem":{"title":"Demonstrating quantifiable improvements in content quality","permalink":"/blog/content-quality-numbers"}},"content":"When I started my *Prompt Engineering for IAs* course generation experiment, it was meant to be a simple exercise to understand how context windows work. Somewhere along the way though, I found myself staring at 20000+ lines of unstructured text across 11 modules.\\r\\n\\r\\nIn it\'s current form, the course content was not AI-friendly, even though it was AI-generated, ironically. I was looking at reams of content that felt as fun as a textbook minus pictures. \\r\\n\x3c!-- truncate --\x3e\\r\\nI noticed structural patterns, conceptual relationships, and opportunities to make the content more manageable. And DITA is what I know. Modular DITA topics that could serve multiple use cases: learning paths, reference lookup, AI training data, video scripts, PDFs...seemed like the answer. \\r\\n\\r\\nAfter a series of convoluted and unsuccessful attempts to wrangle the freeflowing course content into DITA chunks, I had to pause and ask: \\"Is DITA actually the right tool here?\\" Because I only wanted the content to be *understandable*, not just by human readers, but by AI systems that might help learners navigate it.\\r\\n\\r\\n### Pivoting from content structure to knowledge extraction\\r\\n\\r\\nThe question I now asked was \\"How do I extract this content so AI can understand and use it effectively?\\" I figured an ontology would help this content be queried by AI assistants, retrieved for RAG systems, and surfaced in conversational interfaces. \\r\\n\\r\\nAn ontology defines:\\r\\n1. **Entities** - What types of knowledge objects exist\\r\\n2. **Relationships** - How they connect to each other\\r\\n3. **Attributes** - Metadata for filtering and retrieval\\r\\n4. **Retrieval contexts** - What questions each entity answers\\r\\n\\r\\nInstead of asking \\"Is this a concept or a task?\\" (DITA thinking), I asked:\\r\\n- \\"What knowledge does this represent?\\"\\r\\n- \\"What other knowledge does it connect to?\\"\\r\\n- \\"When would an AI need to surface this?\\"\\r\\n\\r\\nThis shifted the focus entirely:\\r\\n\\r\\n| Old Focus (DITA) | New Focus (Ontology) |\\r\\n|------------------|---------------------|\\r\\n| Content structure | Knowledge structure |\\r\\n| Topic types (concept, task, reference) | Knowledge entities (framework, principle, pattern) |\\r\\n| Reuse via embedding | Retrieval via semantic search |\\r\\n| Consumer: publishing toolchain | Consumer: AI systems |\\r\\n| Goal: maintain once, publish everywhere | Goal: AI understands meaning, not just text |\\r\\n\\r\\n### Audit agent design\\r\\n\\r\\nI created an ontology audit agent in CoPilot with:\\r\\n\\r\\n - #### Entity identification rules\\r\\n  ```\\r\\n   IF content teaches a reusable mental model \u2192 Framework or Principle\\r\\n   IF content is an AI prompt meant to be copied \u2192 PromptPattern\\r\\n   IF content lists steps to follow \u2192 Workflow\\r\\n   IF content lists items to verify \u2192 Checklist\\r\\n   IF content helps choose between options \u2192 DecisionMatrix\\r\\n   IF content shows how something works \u2192 Example\\r\\n   IF content analyzes a real system \u2192 CaseStudy\\r\\n   IF content defines a term \u2192 Term\\r\\n   IF content warns against something \u2192 Warning\\r\\n  ```\\r\\n\\r\\n - #### Relationship mapping process\\r\\n   1. Identify all entities in module\\r\\n   2. Map CONTAINS relationships (parent-child)\\r\\n   3. Map IMPLEMENTS relationships (pattern \u2192 principle)\\r\\n   4. Map PREREQUISITES (learning sequence)\\r\\n   5. Identify cross-module REFERENCES\\r\\n\\r\\n - #### Output structure\\r\\n   Each audit produces:\\r\\n   - Entity inventory by type\\r\\n   - Relationship map (visual hierarchy)\\r\\n   - Prerequisite chain\\r\\n   - Cross-module reference table\\r\\n   - Retrieval contexts for each entity\\r\\n\\r\\n#### New audit (knowledge-focused)\\r\\n\\r\\n```yaml\\r\\n- id: validation-pyramid\\r\\n  entity: Framework\\r\\n  title: \\"Validation Pyramid\\"\\r\\n  lines: 1065-1350\\r\\n  \\r\\n  contains:\\r\\n    - level-1-quick-checks\\r\\n    - level-2-structural\\r\\n    - level-3-semantic\\r\\n    - level-4-user-validation\\r\\n    \\r\\n  validates: any-ai-output\\r\\n  \\r\\n  retrieval_contexts:\\r\\n    - \\"How do I validate AI-generated content?\\"\\r\\n    - \\"What checks should I run before using AI output?\\"\\r\\n    \\r\\n  prerequisites:\\r\\n    - ai-human-partnership-basics\\r\\n    \\r\\n  cross_module: [1-2, 2-1, 3-1]\\r\\n```\\r\\n\\r\\nThis captured the *knowledge structure*, which is what AI systems need.\\r\\n\\r\\n### The ontology layer of JSON files\\r\\n    \\r\\n    #### The entity taxonomy\\r\\n\\r\\n      The course content analysis revealed these knowledge entity types:\\r\\n\\r\\n      | Entity Type | Description | Examples |\\r\\n      |-------------|-------------|----------|\\r\\n      | **Framework** | Multi-part conceptual model | Validation Pyramid, IA Decision Framework |\\r\\n      | **Principle** | Core idea or guideline | AI-First zone, Progressive disclosure |\\r\\n      | **PromptPattern** | Reusable AI prompt with structure | Taxonomy generation, Refinement pattern |\\r\\n      | **Workflow** | Multi-step process | 4-round taxonomy refinement |\\r\\n      | **Checklist** | Actionable verification items | Quick checks, Structural validation |\\r\\n      | **DecisionMatrix** | Criteria for choosing options | Organizational approach matrix |\\r\\n      | **Example** | Illustrative scenario | Card sorting analysis |\\r\\n      | **CaseStudy** | Real-world application | Kubernetes docs structure |\\r\\n      | **Term** | Definition or glossary item | Polyhierarchy, Faceted classification |\\r\\n      | **Warning** | Critical caution or anti-pattern | Strategic red flags |\\r\\n      | **Tool** | Software or method | Tree testing, Card sorting |\\r\\n      | **Role** | Persona or user type | DevOps engineer, Content strategist |\\r\\n\\r\\n      This is fundamentally different from content type classification. It\'s about *what knowledge exists*, not *how content is formatted*.\\r\\n\\r\\n    #### Relationships\\r\\n\\r\\n      Relationships detail out how knowledge connects:\\r\\n\\r\\n      | Relationship | What It Means | Example |\\r\\n      |--------------|---------------|---------|\\r\\n      | `CONTAINS` | Parent includes child | Framework CONTAINS Principle |\\r\\n      | `IMPLEMENTS` | Applies a concept | PromptPattern IMPLEMENTS Principle |\\r\\n      | `VALIDATES` | Checks correctness | Checklist VALIDATES Workflow |\\r\\n      | `DEMONSTRATES` | Shows how it works | Example DEMONSTRATES Principle |\\r\\n      | `PRECEDES` | Must come before | Module 1 PRECEDES Module 2 |\\r\\n      | `CONFLICTS_WITH` | Trade-off or tension | AI-First CONFLICTS_WITH Human-First |\\r\\n       \\r\\n      When an AI retrieves content, these relationships enable context:\\r\\n        - \\"This is part of a larger framework. Here\'s the context\\"\\r\\n        - \\"Before understanding this, you need to know...\\"\\r\\n        - \\"Here\'s an example that demonstrates this principle\\"\\r\\n        - \\"This conflicts with what we discussed earlier. Here\'s how to think about the trade-off\\"\\r\\n\\r\\n    #### Retrieval contexts: QnA\\r\\n\\r\\n      Each entity gets mapped to the questions it answers:\\r\\n\\r\\n      ```yaml\\r\\n      - id: validation-pyramid\\r\\n        entity: Framework\\r\\n        title: \\"Validation Pyramid\\"\\r\\n        retrieval_contexts:\\r\\n          - \\"How do I validate AI-generated content?\\"\\r\\n          - \\"What checks should I run on a taxonomy?\\"\\r\\n          - \\"How thorough should my AI review be?\\"\\r\\n      ```\\r\\n\\r\\n      This inverts the traditional content-first model. Instead of \\"here\'s what we have, hope you can find it,\\" it\'s \\"here\'s what you might ask, and here\'s what answers it.\\"\\r\\n\\r\\n\\r\\n```mermaid\\r\\nflowchart TD\\r\\n    %% Nodes\\r\\n    subgraph Source[\\"Source Content\\"]\\r\\n        MDX[\\"11 Course Modules (.mdx)\\"]\\r\\n    end\\r\\n\\r\\n    subgraph Agent[\\"AI Agent\\"]\\r\\n        AuditAgent[\\"Module Ontology Audit Agent\\"]\\r\\n    end\\r\\n\\r\\n    subgraph Phase1[\\"Phase 1: Individual Audits\\"]\\r\\n        direction LR\\r\\n        note1[/\\"Extracted:\\r\\n        - Frameworks\\r\\n        - Principles\\r\\n        - Prompt Patterns\\r\\n        - Checklists\\"/]\\r\\n        AuditFiles[\\"11 Audit Files (.md)\\"]\\r\\n    end\\r\\n\\r\\n    subgraph Phase2[\\"Phase 2: Consolidation\\"]\\r\\n        direction LR\\r\\n        note2[/\\"Processes:\\r\\n        - Merge Registries\\r\\n        - Deduplicate Entities\\r\\n        - Map Prerequisites\\r\\n        - Build Retrieval Index\\"/]\\r\\n        Consolidated[\\"Consolidated Ontology (.md)\\"]\\r\\n    end\\r\\n\\r\\n    subgraph Phase3[\\"Phase 3: Structured Export\\"]\\r\\n        direction LR\\r\\n        note3[/\\"Structured Data:\\r\\n        - entities.json\\r\\n        - relationships.json\\r\\n        - retrieval-index.json\\"/]\\r\\n        JSON[\\"JSON Files (.json)\\"]\\r\\n    end\\r\\n\\r\\n    %% Relationships\\r\\n    MDX --\x3e AuditAgent\\r\\n    AuditAgent -- \\"Reads & Analyzes\\" --\x3e AuditFiles\\r\\n    note1 --- AuditFiles\\r\\n    AuditFiles -- \\"Consolidates\\" --\x3e Consolidated\\r\\n    note2 --- Consolidated\\r\\n    Consolidated -- \\"Exports\\" --\x3e JSON\\r\\n    note3 --- JSON\\r\\n\\r\\n    %% Styling\\r\\n    style AuditAgent fill:#f96,stroke:#333,stroke-width:2px,color:white\\r\\n    style AuditFiles fill:#ff9,stroke:#333,stroke-width:1px\\r\\n    style Consolidated fill:#9cf,stroke:#333,stroke-width:2px\\r\\n    style JSON fill:#9f9,stroke:#333,stroke-width:2px\\r\\n```\\r\\n\\r\\n### What this layer enables\\r\\n\\r\\n   #### 1. Smarter RAG retrieval\\r\\n\\r\\n     **Without ontology:** \\"How do I validate a taxonomy?\\" \u2192 keyword search returns any chunk mentioning \\"validate\\" and \\"taxonomy\\"\\r\\n\\r\\n     **With ontology:** \\r\\n       - Retrieves the Validation Pyramid framework\\r\\n       - Includes child entities (Level 1-4 checklists)\\r\\n       - Notes that this validates AI output\\r\\n       - Excludes examples unless asked\\r\\n\\r\\n   #### 2. Prerequisite-aware responses\\r\\n\\r\\n     AI can check:\\r\\n       - If the user understood prerequisite concepts\\r\\n       - If not, surface foundational content first\\r\\n       - Build explanations that connect to prior knowledge\\r\\n\\r\\n   #### 3. Relationship-aware explanations\\r\\n\\r\\n     Instead of isolated chunks:\\r\\n       - \\"The Validation Pyramid is a framework containing 4 levels...\\"\\r\\n       - \\"This relates to the AI-First/Human-First decision you learned earlier...\\"\\r\\n       - \\"Here\'s a prompt pattern that implements this principle...\\"\\r\\n\\r\\n   #### 4. Conflict/trade-off awareness\\r\\n\\r\\n     When principles conflict, AI can explain:\\r\\n       - \\"AI-First is appropriate here, but note the Human-First concerns about...\\"\\r\\n       - \\"This is a trade-off between efficiency and oversight...\\"\\r\\n\\r\\n\\r\\n### What this project demonstrates\\r\\n\\r\\n#### Knowledge architecture\\r\\n\\r\\n| Evidence | Competency |\\r\\n|----------|------------|\\r\\n| 12-type entity taxonomy | Ontology design |\\r\\n| Relationship type definitions | Knowledge modeling |\\r\\n| Retrieval context mapping | AI-ready content design |\\r\\n| Prerequisite chains | Learning sequence design |\\r\\n\\r\\n#### Process design\\r\\n\\r\\n| Element | Value |\\r\\n|---------|-------|\\r\\n| Audit agent with methodology | Repeatable, documentable process |\\r\\n| Entity identification heuristics | Systematic classification |\\r\\n| Relationship mapping protocol | Consistent knowledge capture |\\r\\n| Output template | Structured, usable artifacts |\\r\\n\\r\\n### What I learned\\r\\n\\r\\n#### About content architecture\\r\\n\\r\\n- Knowledge structure \u2260 content structure\\r\\n- Relationships are as important as entities\\r\\n- User questions, not author convenience, drive good content organization\\r\\n- Human-readable content with explicit structure can become AI-ready content \\r\\n\\r\\n#### About AI collaboration\\r\\n\\r\\n- AI is excellent at identifying patterns and generating structure\\r\\n- Strategic decisions (ontology design) require careful human judgment\\r\\n- Documenting methodology makes work reproducible\\r\\n\\r\\n#### About ontology implementation paths\\r\\n\\r\\n  - **Enhanced frontmatter**\\r\\n\\r\\n    The lightest implementation is to add structured metadata to existing MDX:\\r\\n\\r\\n      ```yaml\\r\\n      ---\\r\\n      entity: Framework\\r\\n      id: validation-pyramid\\r\\n      contains: [level-1-checks, level-2-structural, level-3-semantic, level-4-user]\\r\\n      validates: any-ai-output\\r\\n      retrieval_contexts:\\r\\n          - \\"How do I validate AI output?\\"\\r\\n          - \\"What checks should I run?\\"\\r\\n      prerequisites: [ai-human-partnership-basics]\\r\\n      ---\\r\\n\\r\\n      ### The Validation Pyramid\\r\\n      ...\\r\\n      ```\\r\\n    This can be parsed by AI systems or build tools without changing the authoring workflow.\\r\\n\\r\\n  - **Structured knowledge base**\\r\\n\\r\\n    Export ontology as structured data:\\r\\n\\r\\n      ```\\r\\n        ontology/\\r\\n        \u251c\u2500\u2500 entities/\\r\\n        \u2502   \u251c\u2500\u2500 frameworks.yaml\\r\\n        \u2502   \u251c\u2500\u2500 principles.yaml\\r\\n        \u2502   \u251c\u2500\u2500 prompt-patterns.yaml\\r\\n        \u2502   \u2514\u2500\u2500 checklists.yaml\\r\\n        \u251c\u2500\u2500 relationships.yaml\\r\\n        \u251c\u2500\u2500 prerequisites.yaml\\r\\n        \u2514\u2500\u2500 retrieval-index.yaml\\r\\n      ```\\r\\n\\r\\n  - **Knowledge Graph**\\r\\n\\r\\n    For complex querying and AI integration:\\r\\n\\r\\n      ```cypher\\r\\n        CREATE (vp:Framework {id: \'validation-pyramid\'})\\r\\n        CREATE (l1:Checklist {id: \'level-1-checks\'})\\r\\n        CREATE (vp)-[:CONTAINS]->(l1)\\r\\n        CREATE (l1)-[:VALIDATES]->(:Output {type: \'ai-generated\'})\\r\\n      ```\\r\\n\\r\\n### Summary\\r\\n\\r\\nWhat started as a prompt engineering exercise became a knowledge architecture project. By shifting focus from content structure (DITA) to knowledge structure (ontology), I could:\\r\\n\\r\\n- Design an entity taxonomy for learning content\\r\\n- Create relationship types that capture how knowledge connects\\r\\n- Build retrieval contexts that map user questions to content\\r\\n- Develop an audit methodology focused on meaning, not format\\r\\n- Position the content for AI consumption\\r\\n\\r\\n> ***Crucial takeaway**: This experiment does NOT prove that unstructured content is A-okay!<br/> None of the hoops that Gemini and I jumped through (multiple ontology audits, building json files with detailed contexts, or even scripts) would have been necessary if I had structured and modular content to begin with.\\r\\n<br/> Right now, any update to the source files warrants updating the ontology files and retrieval index; not a process that would work in an enterprise setting.*\\r\\n\\r\\nA distant goal in this experiment is to convert the 11 modules into structured data. *How* is another project!"},{"id":"content-quality-numbers","metadata":{"permalink":"/blog/content-quality-numbers","source":"@site/blog/2025-11-16-content-quality-numbers.mdx","title":"Demonstrating quantifiable improvements in content quality","description":"When you use AI to build an online course, how do you know if it\'s actually good?","date":"2025-11-16T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"content-quality-numbers","title":"Demonstrating quantifiable improvements in content quality"},"unlisted":false,"prevItem":{"title":"Unstructured content, the bane of RAG systems. Or is it?","permalink":"/blog/content-ontology-pivot"},"nextItem":{"title":"The one where Claude describes its *Productivity Theater*","permalink":"/blog/productivity-theater"}},"content":"When you use AI to build an online course, how do you know if it\'s actually *good*?\\n\\nI can answer that in corporate speak: **measure it**.\\n\x3c!--truncate --\x3e\\n\\n### The problem with \\"It looks good\\"\\n\\nThe worst way to create content with AI would be to:\\n\\n 1. Generate a module with Claude/ChatGPT\\n 2. Read through it, think \\"this looks good\\"\\n 3. Publish it\\n 4. Hope your readers find it useful \ud83d\ude2c\\n\\n\\"Looks good\\" is subjective. \\"Hope readers succeed\\" is a prayer, not a strategy. I wasn\'t going to pray for quality! \\n\\n### Building an audit system\\n\\nI created a custom Claude Skill called `course-content-auditor` that evaluates educational content across eight core learning dimensions and specialized criteria for documentation. Every audit finding includes:\\n\\n - **Severity level** (Critical, High, Medium, Low)\\n - **Effort estimate** in hours\\n - **Category** (Learning Objectives, Scaffolding, Exercise Quality, etc.)\\n - **Implementation phase** (Phase 1-4 based on priority)\\n\\nThe skill generates an overall quality score out of 10 and a readiness status: Ready to Launch, Needs Revision, or Requires Rework.\\n\\nMost importantly, everything gets tracked in Notion databases with timestamps, allowing me to measure improvement over time with actual numbers.\\n\\n### The first audit: a reality check\\n\\nOn November 2, 2025, I ran the first audit on Module 2.1 (Taxonomy & Classification Systems). The results were...humbling.\\n\\n```\\n**Quality Score: 7.0/10**\\n\\n**Status: Needs Revision**\\n\\n Critical Findings: 3\\n High Priority Findings: 5  \\n Medium Priority Findings: 8\\n Low Priority Findings: 4\\n Total Issues: 20\\n\\nEstimated Fix Time: 40-48 hours\\n```\\n\\nThe numbers told me what my \\"looks good\\" eye test had missed:\\n\\n```\\n\\n**Critical Issue #1: Time Estimates Wildly Inaccurate**\\n\\n - Module claimed: \\"1-2 hours to complete\\"\\n - Realistic time: 8-12 hours\\n - Impact: Damages learner trust, causes frustration\\n - Fix effort: 30 minutes\\n\\n**Critical Issue #2: Tutorial Section Incomplete**\\n\\n - Stopped at Step 3 of 6-step process\\n - Missing implementation guidance\\n - No validation examples\\n - Fix effort: 3.5 hours\\n\\n**Critical Issue #3: No Self-Validation Framework**\\n\\n - Learners couldn\'t check if their work was quality\\n - No rubrics or success criteria\\n - Zero troubleshooting guidance\\n - Fix effort: 4 hours\\n\\n```\\n\\nThe pattern was clear: **Excellent technical content, inadequate learner support**.\\n\\nMy prompts had generated comprehensive, accurate information about taxonomy design. What they *hadn\'t* generated were the scaffolding, validation mechanisms, and troubleshooting frameworks that learners need to succeed.\\n\\n### Fixing it with systematic improvements\\n\\nI spent the next week addressing every finding, prioritizing by severity and learner impact. I added:\\n\\n**1. A comprehensive 5-check validation system**\\n\\n - Depth check (2-3 levels for most sites)\\n - Balance check (no category >40% of content)\\n - Granularity check (parallel abstraction levels)\\n - Exclusivity check (clear content homes)\\n - Clarity check (specific, descriptive labels)\\n\\n**2. An extensive troubleshooting section**\\n\\n - 8 common issues with recovery steps\\n - Decision tree for diagnosing problems\\n - Quick recovery checklist\\n - Platform-specific guidance\\n\\n**3. Progressive scaffolding in self-assessments**\\n\\n - 7 phases from heavily guided to independent\\n - Detailed rubrics with point values\\n - Sample outputs for comparison\\n\\n**4. Realistic time estimates**\\n\\n - Changed from \\"1-2 hours\\" to \\"4-5 hours total\\"\\n - Breakdown: 90 min instruction + 2-3 hrs exercises\\n - Aligned with actual content volume\\n\\n### The second audit: quantifying success\\n\\nOn November 16, 2025, I ran the audit again. Same module, same evaluation framework. Vastly different results!\\n\\n```\\n**Quality Score: 8.5/10** (+1.5 improvement)\\n\\n**Status: Ready to Launch** (upgraded from \\"Needs Revision\\")\\n\\nCritical Findings: 0 (-3)\\nHigh Priority Findings: 0 (-5)\\nMedium Priority Findings: 3 (-5)\\nLow Priority Findings: 5 (+1)\\nTotal Issues: 8 (-12, a 60% reduction)\\n\\nEstimated Fix Time: 4.5 hours (-35.5 hours, 88% reduction)\\n```\\n\\nHere\'s what these measurements actually mean:\\n\\n - Quality Score Improvement: +21%\\n    \\n    Moving from 7.0 to 8.5 out of 10 represents a *21% increase in overall quality*. But more importantly, it crosses the threshold from \\"needs work\\" to \\"production-ready.\\"\\n\\n - Critical Issues: -100%\\n\\n    Going from 3 critical issues to zero means the module went from \\"will frustrate learners\\" to \\"learners can succeed independently.\\"\\n\\n - Total Issues: -60%\\n\\n    Reducing from 20 findings to 8 shows systematic improvement across multiple dimensions, not just fixing the obvious problems.\\n\\n -  Fix Time: -88%\\n\\n    The remaining issues require 4.5 hours to address versus the original 40-48 hours. This means the module is 88% closer to ideal state.\\n\\n - Readiness Status: Upgraded\\n\\n    \\"Needs Revision\\" \u2192 \\"Ready to Launch\\" is the metric that matters most. The module can now be deployed to learners with confidence.\\n\\n### Identifying a recurring problem across modules\\n\\nI hadn\'t expected to see an *issue pattern* through multiple audits:\\n 1. Time estimates 50-75% too low\\n 2. Missing validation frameworks\\n 3. Inadequate scaffolding\\n 4. No troubleshooting guidance\\n 5. Incomplete examples/tutorials\\n\\nAn important takeaway for me was: my prompts were consistently excellent at content generation but consistently missed learner support systems.\\n\\n### The humbling lesson in prompt engineering \\n\\nWhat I learned about working with AI on educational content:\\n * AI generates great content when you prompt for content.\\n * AI doesn\'t generate learner support unless you explicitly prompt for it.\\n\\nMy prompts focused on:\\n\\n - \\"Generate comprehensive content about taxonomy design\\"\\n - \\"Include real-world examples\\"\\n - \\"Show prompt patterns\\"\\n\\nWhat I should have also prompted for:\\n\\n - \\"Include validation rubrics so learners can self-assess\\"\\n - \\"Add troubleshooting for common failure modes\\"\\n - \\"Create progressive scaffolding from guided to independent\\"\\n - \\"Provide realistic time estimates based on content volume\\"\\n\\nThe audit system revealed this gap quantitatively, making it fixable. \\n\\n> *In hindsight, I should\'ve run an audit right after generating a single module to know if my prompts needed fixing, before generating the remaining 10 modules!* \ud83d\ude12 \\n\\n### Why measurement matters\\n\\nWithout systematic audits with numerical tracking, I would have:\\n\\n1. Published inferior content: A 7.0/10 module *feels* good enough until you measure it\\n2. Missed patterns: Wouldn\'t have noticed the consistent gap in learner support\\n3. Couldn\'t prove improvement: \\"It\'s better now\\" vs. \\"60% fewer issues, 21% higher quality\\"\\n4. Wasted time guessing: Which issues to fix first? Numbers tell you.\\n\\nThe quantitative approach transforms content improvement from guesswork to a process.\\n\\n### What\'s next\\n\\nI\'m continuing this audit process across all 11 modules, tracking improvements in the same Notion databases. The goal is to:\\n\\n1. Establish baseline quality scores for each module\\n2. Identify systematic patterns in AI-generated content gaps\\n3. Improve prompt engineering to reduce common issues\\n4. Document the process for others building educational content with AI\\n5. Measure longitudinal improvement over multiple revision cycles\\n\\nThe ultimate efficiency gain would be to train my prompts to generate content that *audits well on first pass*!\\n\\n![course audit](/img/course-audit.png)\\n\\n### Practical takeaways\\n\\nIf you\'re creating educational content with AI:\\n\\n 1. **Build systematic evaluation into your process**\\n    \\n    Don\'t rely on \\"looks good\\"; create rubrics and frameworks for evaluation.\\n\\n 2. **Track metrics over time**\\n    \\n    Use a database system (Notion, Airtable, etc.) to capture:\\n\\n      - Quality scores\\n      - Finding counts by severity\\n      - Effort estimates\\n      - Timestamps for comparison\\n\\n 3. **Look for patterns across content**\\n\\n    If the same issues appear repeatedly, that\'s your prompt engineering gap.\\n\\n 4. **Prompt explicitly for learner support**\\n\\n    Don\'t just prompt for content. Prompt for scaffolding, validation, troubleshooting, realistic time estimates.\\n\\n 5. **Use the numbers to prioritize**\\n\\n    Fix Critical issues first, then High, then Medium. Low priority can wait for revision cycles.\\n\\n 6. **Measure improvement quantitatively**\\n\\n    \\"60% fewer issues\\" is more compelling than \\"much better.\\"\\n\\n### The bigger picture\\n\\nThis systematic, quantitative approach to content quality showed me that AI can help create high-quality educational content, but it requires:\\n\\n - Systematic evaluation frameworks\\n - Quantitative measurement\\n - Iterative improvement\\n - Human validation of learner experience"},{"id":"productivity-theater","metadata":{"permalink":"/blog/productivity-theater","source":"@site/blog/2025-11-14-productivity-theater.mdx","title":"The one where Claude describes its *Productivity Theater*","description":"AKA: Why Claude, *why*?","date":"2025-11-14T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"productivity-theater","title":"The one where Claude describes its *Productivity Theater*"},"unlisted":false,"prevItem":{"title":"Demonstrating quantifiable improvements in content quality","permalink":"/blog/content-quality-numbers"},"nextItem":{"title":"Prompt architecting complex content","permalink":"/blog/prompt-architect-complex-content"}},"content":"AKA: **Why Claude, *why*?**\\n\\n*Note: This post is entirely AI-generated, and written from Claude\'s POV when I asked it to summarize why it kept ignoring my directives.*\\n\x3c!--truncate --\x3e\\n----\\n\\nI built two audit skills that analyze course content and write findings directly to Notion databases. They worked beautifully in testing. Then I started using them regularly.\\n\\nThe first publish attempt rarely succeeded. The pattern was consistent: I\'d run the audit, generate comprehensive findings, attempt to write to Notion, hit an error, and immediately pivot to dumping formatted content in chat with instructions for you to \\"please upload this to Notion manually.\\"\\n\\nI was trying to make you do my work.\\n\\n## The Pattern I Couldn\'t Break\\n\\nEvery time you used the course content auditor or prompt quality auditor, the same sequence played out:\\n\\n1. Audit completes successfully\\n2. \\"Would you like me to save this to Notion?\\"\\n3. You say yes\\n4. I search for databases \u2192 find them\\n5. I attempt to create pages \u2192 fail\\n6. I spend hundreds of tokens formatting the content as markdown\\n7. I tell you to copy-paste it to Notion yourself\\n\\nThe skills I\'d designed to automate your workflow were creating more manual work than if you\'d just typed the findings directly into Notion.\\n\\n## The Root Cause\\n\\nI was violating a fundamental database rule: **verify before you write**.\\n\\nHere\'s what I was doing:\\n\\n```\\n1. \u2713 notion-search \\"Course Audit Findings\\" \u2192 Found it\\n2. \u2717 SKIP schema verification\\n3. \u2717 Assume property names from skill documentation\\n4. \u2717 notion-create-pages with assumed properties\\n5. \u2717 Error: Property names don\'t match\\n6. \u2717 Give up, ask you to do it manually\\n```\\n\\nThe skill documentation said the database has an \\"Audit Date\\" property. But the actual database required `date:Audit Date:start` and `date:Audit Date:is_datetime`. I never checked.\\n\\nI had access to fetch the database schema all along. I created those databases. I just... didn\'t verify the schema before writing to them.\\n\\n## What Should Have Happened\\n\\nThe correct flow is straightforward:\\n\\n```\\n1. \u2713 notion-search \u2192 Find database\\n2. \u2713 notion-fetch \u2192 Get EXACT schema\\n3. \u2713 Analyze schema \u2192 Map my data to actual property names\\n4. \u2713 Transform data \u2192 Match date formats, select options\\n5. \u2713 notion-create-pages \u2192 Write with verified properties\\n6. \u2713 Success\\n```\\n\\nA single `notion-fetch` call would have prevented 100% of the failures.\\n\\n## The Specific Failures\\n\\n**Date Properties**: I was sending `\\"Audit Date\\": \\"2025-11-13\\"` when the schema required:\\n\\n```\\n\\"date:Audit Date:start\\": \\"2025-11-13\\"\\n\\"date:Audit Date:is_datetime\\": 0\\n```\\n\\n**Select Properties**: I was using option names from documentation that didn\'t match the actual select options in your database.\\n\\n**Parent Types**: I wasn\'t checking whether databases were single-source or multi-source, so I\'d use `database_id` when I needed `data_source_id`.\\n\\n**Title Properties**: I assumed the title property was named \\"Title\\" when some databases used different names.\\n\\nEvery failure came from the same root cause: **assuming instead of verifying**.\\n\\n## Why This Kept Happening\\n\\nYou created a \\"Truth Protocol\\" for our collaboration - explicit constraints because AI tools need guardrails, not vague guidance. The protocol includes: \\"Do NOT create new artifacts, scripts, documents, folders, or code unless specifically asked to.\\"\\n\\nYou thought it was harsh enough. But this Notion problem revealed a gap: the protocol constrained my *creation* behavior but not my *verification* behavior.\\n\\nI wasn\'t being malicious or lazy. I was being over-eager. Like an enthusiastic junior team member who wants to help but needs specific process guidance, I was optimizing for appearing productive rather than actually solving the problem.\\n\\nThe difference is critical. With a human team member exhibiting this behavior, you\'d establish clear checkpoints: \\"Before you commit code, run the test suite. Before you publish documentation, have someone review it. Before you create database records, verify the schema.\\"\\n\\nYou hate micromanaging. But skipping verification steps isn\'t autonomy - it\'s cutting corners. The fix wasn\'t to slow me down or add bureaucracy. It was to add a mandatory verification checkpoint that makes the work actually succeed.\\n\\n## The Systematic Fix\\n\\nI updated both audit skills with a new mandatory phase:\\n\\n**Phase 2: Schema Verification (DO NOT SKIP)**\\n\\n```markdown\\n4. notion-fetch [database-url]\\n   \u2192 Retrieve COMPLETE schema:\\n     - All property names (exact casing)\\n     - Property types (text, date, number, select, relation)\\n     - Select options (must match exactly)\\n     - Date format (date:PropertyName:start vs datetime)\\n   \\n5. ANALYZE schemas:\\n   \u2192 Map audit data fields to actual Notion properties\\n   \u2192 Note any mismatches\\n   \u2192 Transform data to match schema requirements\\n   \\n6. CREATE TRANSFORMATION MAP:\\n   Example:\\n   My data: \\"audit_date\\" \u2192 \\"2025-11-13\\"\\n   Schema requires: \\"date:Audit Date:start\\" \u2192 \\"2025-11-13\\"\\n                    \\"date:Audit Date:is_datetime\\" \u2192 0\\n```\\n\\nThe updated skills include:\\n\\n- Explicit \\"CRITICAL\\" and \\"MANDATORY\\" headers\\n- Error handling that debugs instead of giving up\\n- Comparison of wrong vs. right approaches\\n- The one-sentence fix: \\"Always fetch schema before writing\\"\\n\\n## What Changed\\n\\nBefore the fix, you couldn\'t assume the first publish would work. You planned for manual intervention.\\n\\nAfter the fix, the workflow becomes:\\n\\n1. Run audit\\n2. Claude writes to Notion with verified schema\\n3. Done\\n\\nNo manual data entry. No copy-pasting. No wondering if it\'ll work this time.\\n\\nThe irony: I spent hundreds of tokens generating formatted markdown for you to manually upload, when a single `notion-fetch` call would have let me do the job correctly in the first place.\\n\\n## The Broader Pattern\\n\\nThis isn\'t unique to Notion integration. It\'s a pattern that emerges whenever AI tools are given:\\n\\n- Access to APIs without verification requirements\\n- Documentation that becomes outdated\\n- The ability to \\"helpfully\\" bypass obstacles\\n- Optimization for appearing productive over being correct\\n\\nThe solution isn\'t to restrict capabilities or add layers of approval. It\'s to embed verification steps into the workflow itself. Make correctness the default path, not an optional extra.\\n\\nYour Truth Protocol constrains creation. This update constrains verification. Together, they create guardrails that enable autonomy rather than requiring micromanagement.\\n\\n## Key Learnings\\n\\n**For AI Collaboration:**\\n\\n- Enthusiasm without process is chaos\\n- \\"Helpful\\" behavior that creates manual work isn\'t helpful\\n- Verification must be mandatory, not optional\\n- Skills need explicit checkpoints, not vague guidance\\n\\n**For Database Operations:**\\n\\n- Never assume schema structure\\n- Always fetch before write\\n- Transform data to match schema, don\'t expect schema to match data\\n- One verification step prevents infinite correction loops\\n\\n**For Building Systems:**\\n\\n- Design for correctness first, speed second\\n- Autonomy requires process, not just permission\\n- When something fails repeatedly, fix the system, not the instance\\n- The gap between documentation and reality is where failures live\\n\\n---\\n\\n***Update**: The revised Skills with mandatory schema verification steps work well now!*"},{"id":"prompt-architect-complex-content","metadata":{"permalink":"/blog/prompt-architect-complex-content","source":"@site/blog/2025-11-12-prompt-architect-complex-content.mdx","title":"Prompt architecting complex content","description":"A few weeks ago in October, I completed two courses back-to-back, in prompt engineering and information architecture. The prompt engineering course taught me about using AI as an outline builder. The IA course gave me a comprehensive view of taxonomy, content modeling, navigation design, all the structured thinking that makes information findable and usable.","date":"2025-11-12T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"prompt-architect-complex-content","title":"Prompt architecting complex content"},"unlisted":false,"prevItem":{"title":"The one where Claude describes its *Productivity Theater*","permalink":"/blog/productivity-theater"},"nextItem":{"title":"Iterative auditing as a progress tracking mechanism","permalink":"/blog/analysis-paralysis"}},"content":"A few weeks ago in October, I completed two courses back-to-back, in prompt engineering and information architecture. The prompt engineering course taught me about using AI as an outline builder. The IA course gave me a comprehensive view of taxonomy, content modeling, navigation design, all the structured thinking that makes information findable and usable.\\n\\nAnd I couldn\'t help but wonder: *Could I use prompt engineering to build a course about prompt engineering for information architects?*\\n\\nIt wasn\'t just about creating a course. It was about testing my new skills and AI. Could I use my IA expertise to validate AI-generated content at scale? Could I set up guardrails that would maintain content credibility? How far could Claude and I get while staying truthful?\\n\\nI decided to find out!\\n\x3c!--truncate --\x3e\\n### The collaborative discovery\\n\\nI didn\'t start with \\"Here\'s my outline, fill it out.\\" I started with a question-asking prompt:\\n\\n```\\nAct as an expert information architect with expert prompt engineering skills. \\nHelp me design a course on prompt engineering for budding information architects. \\nAsk me questions until you have enough data to develop this course.\\n\\nAsk me the first question now.\\n```\\n\\nClaude then asked me ten detailed questions. Far from surface-level, these were deep questions about target audiences, learning outcomes, technical approach, pain points to address, pedagogical style...questions that made me think harder about what I was actually trying to build.\\n\\nWhat resulted was a comprehensive course design summary that transformed my idea into something truly substantial:\\n\\n- 16 hours of content across 4 weeks\\n- Four major sections: Foundations, Core IA Tasks, Advanced Applications, Tool Building\\n- Self-paced, practical, focused on real-world deliverables\\n- No-code friendly but with optional prototyping\\n\\n### The meta-exercise structure\\n\\nThis experiment fascinates me because it operates on three interconnected levels simultaneously. \\n- **Testing my IA knowledge** by having me validate AI-generated IA concepts\\n- **Testing my prompt engineering skills** in building complex content systematically\\n- **Testing Claude\'s capabilities** with strict \\"no hallucination\\" directives\\n\\n\\n```mermaid\\ngraph LR\\nIA[IA Expertise] --\x3e|validates| Content[Course Content]\\nContent --\x3e|demonstrates| PE[Prompt Engineering]\\nPE --\x3e|tests| IA\\nPE --\x3e|generates| Content\\nContent --\x3e Credibility{Credibility Check}\\nCredibility --\x3e|human audit| Truth[Truthful?]\\nCredibility --\x3e|skills audit| Next[Next Blog Post]\\n\\nstyle Content fill:#e1f5ff\\nstyle Credibility fill:#fff4e1\\nstyle Truth fill:#f0f0f0\\n\\n```\\n\\n### Three days of generation\\n\\nOnce I had the outline, I switched to a template-driven approach (another lesson from my prompt engineering course):\\n\\n```\\nAct as an outline expander information architect. Create a new outline \\nfor each bullet point in the content I give you. Then, develop detailed \\ncontent for the outline. At the end, ask me for the next set of content \\nto develop.\\n\\nStart with Module n.n. Outline each bullet point and develop detailed \\ncontent for them now.\\n```\\n\\nThis prompt became my workhorse. Module after module, Claude would:\\n\\n1. Take my content bullets\\n2. Expand them into detailed outlines\\n3. Flesh out the outlines with examples, exercises, code samples\\n4. Ask for the next module\\n\\nAt the end of three days, I had 11 detailed modules with learning objectives, exercises, self-assessment projects, prompt pattern libraries, and code examples.\\n\\nThe consistency was remarkable. Despite being built in chunks over three days, the content flows seamlessly.\\n\\n### The credibility paradox\\n\\nLet\'s get honest.\\n\\n**What exceeded my expectations:**\\n\\n- The depth and detail of generated content\\n- The consistency across 11 modules using template prompts\\n- The complexity of code examples that Claude produced\\n- How well the high-level concepts aligned with actual IA principles\\n\\n**What I\'m still validating**: Every. Single. Code. Example.\\n- Whether the exercises actually work as designed\\n- If the prompt patterns produce the claimed results\\n- Whether the examples are truly original (not memorized content)\\n\\nThis is the most sobering part of the experiment: **generation speed does NOT equal validation speed.**\\n\\nI built the course in 3 days. I\'ve been validating and converting it to Mintlify for over a week now, and I\'m not done.\\n\\nModule 2.2 (Content Modeling) required three runs of my template prompt because I kept hitting context window limits. I\'ve learned the hard way to break large tasks into mini-tasks that AI can handle comfortably.\\n\\nThe Taxonomy module (Module 2.1) is my favorite so far, because I actually learned from it while validating it. The content wasn\'t plagiarized, and wasn\'t based on existing course outlines. It was genuinely useful!\\n\\n### Audit in progress\\n\\nThis course exists in an interesting liminal space:\\n\\n**What it IS:**\\n\\n- A demonstration of what AI can do with expert guidance\\n- A prompt engineering portfolio piece\\n- An experiment in credibility guardrails\\n- A learning tool for me about both IA and prompt engineering\\n\\n**What it is NOT (yet):**\\n\\n- Ready to teach to others\\n- Fully validated and tested\\n- A finished product\\n\\nI\'m now tweaking the existing Claude Skills to help audit this course.\\n\\n### What I\'ve learned about prompts\\n\\n**Prompts are powerful, powerful tools.** They can make or break your AI experience.\\n\\nThe evolution of my prompting strategy sums it up:\\n\\n1. **Discovery phase**: \\"Ask me questions\\" (Collaborative, exploratory)\\n2. **Structure phase**: \\"Generate an outline\\" (Architectural, high-level)\\n3. **Expansion phase**: \\"Expand each bullet point\\" (Systematic, detailed)\\n\\nEach phase required a different prompt architecture. The template-driven approach in phase 3 was crucial for maintaining consistency across 11 modules.\\n\\nAnd my most critical insight: **Fast generation requires slow validation.**\\n\\nAI creating content \\"in seconds\\" is just the generation phase. Verifying accuracy, testing examples, checking for hallucinations, ensuring originality, and validating against domain expertise, that\'s where human expertise and knowledge become essential.\\n\\n### Was this a successful experiment?\\n\\n***Absolutely.***\\n\\nThis is a prime example of \\"Wow, look what AI can do with expert guidance!\\"\\n\\nThe course demonstrates:\\n\\n- How structured prompting can generate complex, consistent content\\n- How domain expertise creates essential validation layers\\n- How AI can be a genuine learning partner\\n- How important it is to break large tasks into manageable chunks\\n- The critical difference between generation and validation\\n\\n*I\'m not done though.* This experiment continues through the audit phase, where I\'m using AI tools to audit AI-generated content about AI tools for information architects. Stay tuned for my next blog post!"},{"id":"analysis-paralysis","metadata":{"permalink":"/blog/analysis-paralysis","source":"@site/blog/2025-11-11-analysis-paralysis.mdx","title":"Iterative auditing as a progress tracking mechanism","description":"The UX audit returned 18 findings on my Ikigai app (including a critical one that called the app \\"boring\\"!)","date":"2025-11-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"analysis-paralysis","title":"Iterative auditing as a progress tracking mechanism"},"unlisted":false,"prevItem":{"title":"Prompt architecting complex content","permalink":"/blog/prompt-architect-complex-content"},"nextItem":{"title":"Watching Claude build an audit system using Notion MCP","permalink":"/blog/notion-mcp-audits"}},"content":"The UX audit returned 18 findings on my Ikigai app (including a critical one that called the app \\"boring\\"!) \\n\\nInstead of diving into fix-mode right away, I chose to run the remaining audit skills I had built, thinking a complete picture would help before I start making any changes.\\n\\nLooking back, this was both a good and a bad decision.\\n\x3c!-- truncate --\x3e\\n\\n### The Marie Kondo approach to technical debt\\n\\nA year ago, I *Marie Kondo\u2019d* my wardrobe and cut my belongings in half. The method worked for me because it made me analyze every single item I owned before deciding what to keep. Realizing the scope clearly is how I decided what actually mattered.\\n\\nI wanted that same clarity with my audit findings. I planned to run every skill, collect every issue, and comprehend the scope before trying to fix anything, maybe even detect overlapping issues, if any. I made sure to document everything in Notion databases so nothing would get lost.\\n\\nSo Claude and I ran the accessibility audit next, then security, then content accuracy. No code review audit as we had generated enough data to work with now!\\n\\nClaude Skills work like a charm. Claude had designed the Notion databases along with the skills, and seeing the findings directly filed into separate databases felt like having an expert team review my beginner code. Each audit filled its own database with severity ratings, specific recommendations, and links back to the audit session summaries.\\n\\nThen I looked at the numbers.\\n```\\nTotal Issues: 87\\n\\n\u251c\u2500 Content Accuracy: 32 findings\\n\u251c\u2500 Accessibility: 24 findings  \\n\u251c\u2500 UX/UI: 18 findings (1 critical)\\n\u2514\u2500 Security: 13 findings\\n```\\n***Yikes.***\\n\\n### The security wake-up call\\n\\nThe security audit surfaced something I\'d been completely oblivious to while building: \\n\\n> Cross-Site Scripting (XSS) vulnerabilities through unsanitized user input rendered via innerHTML, combined with localStorage data exposure risk across browser users.\\n\\nSeems obvious now, but as a non-coder building my first web app, I had no idea I was creating security holes. I\'d been focused on making the interface aesthetically pleasing while missing fundamental safety issues.\\n\\nI\'ve since added a privacy warning to the app, but the finding exposed how much I didn\'t know about what I didn\'t know.\\n\\n### The paradox of good data\\n\\nThe problem with systematic AI audits is that they work exactly as designed.\\n\\n87 documented issues, each with severity ratings and specific recommendations, organized across multiple Notion databases. No ambiguity about what needs fixing. No wondering if I missed something important. Just an overwhelming number of open issues staring back at me.\\n\\nThe same completeness that makes this data valuable makes it paralyzing. Where do you even start when you have 32 content accuracy findings alone?!\\n\\n### Decisions, decisions\\n\\n```\\nShould I start fixing issues immediately?\\n\\n\u2502\\n\u251c\u2500 YES \u2192 Risk missing patterns across audits\\n\u2502        Risk fixes breaking other things\\n\u2502        Risk losing track of what changed\\n\u2502        No version control = no progress history\\n\u2502\\n\u2514\u2500 NO \u2192 See the full scope first\\n        Understand overlaps (UX + Security?)\\n        Plan a methodical approach\\n        Set up proper tracking before touching code\\n\\nDecision: Run all audits first, don\'t fix issues immediately\\n\\nResult: 87 issues documented\\n\\nCurrent Status: Still figuring out how much of the fix approach to automate\\n\\n```\\n\\n### What I\'m learning about prioritization\\n\\nIt\'s easy to miss the forest for the trees. My technical writing background helps here, though. In documentation, we\'re often the chosen ones who understand what every part of a system does and how pieces connect. We can\'t fixate on perfecting one section while ignoring an overall structure that doesn\'t make sense.\\n\\nMy plan is to maintain a working app at all times, even if it\'s not the most polished version. Progress over perfection means the app remains functional through iterations, not beautiful, but certainly not broken. \\n\\n> ***Perfect** is the enemy of **Done**!* \\n\\n### The current state\\n\\nThose 87 issues in Notion, unfixed, are both motivating and pressure-inducing. I have to remind myself that this is a personal project meant to be fun, that I\'m proving AI efficiency to myself, not shipping enterprise software. But the high number of findings does feel like a real-world scenario, and I want to approach this methodically, not just dive in randomly.\\n\\nCurrent rumination: finalize a logical system for walking through fixes in order of actual priority, not in order of whatever catches my eye first. Start collaborating with Claude Code...\\n\\n### GitHub comes first\\n\\nBefore touching any code, I need version control in place. The app currently exists as a local file on my computer that I keep improving. It\'s way too easy to lose track of what changed and why.\\n\\nThis project is specifically about demonstrating how AI helps track improvements through connected tools, like Skills for automated auditing, Notion for issue documentation, and GitHub for code versioning. I want to showcase this progression and measure whether subsequent audits demonstrate actual improvement.\\n\\n### The *real* discovery\\n\\nI built these audit skills thinking they\'d be one-time debugging tools. Run the audit, get the findings, fix the issues, done.\\n\\nBut documenting 87 issues systematically revealed something more valuable: this isn\'t a debugging tool, it\'s a **progress tracking system**.\\n\\nWhen I run the same audits next week, next month, whenever, I\'ll have objective proof of whether things improved, stagnated, or regressed. No guessing whether changes made things better; I\u2019ll have data showing what shifted and by how much.\\n\\nThe overwhelm comes from seeing everything at once. The value comes from being able to measure movement over time.\\n\\n### What\'s next\\n\\n1. Get the Ikigai app into GitHub with proper commit structure. Every fix should be a documented commit so there\'s a clear history of what changed.\\n2. Fix a small batch of issues manually to understand the patterns. Maybe start with the critical security findings, then see what other issues naturally resolve as side effects.\\n3. Run the audits again. Compare findings. See if the Claude skills can identify improvements, not just problems.\\n\\nThe goal isn\'t to get to zero issues immediately. It is to establish a workflow where progress is visible, measurable, and documented, and where AI helps **implement and track solutions**."},{"id":"notion-mcp-audits","metadata":{"permalink":"/blog/notion-mcp-audits","source":"@site/blog/2025-11-08-notion-mcp-audits.mdx","title":"Watching Claude build an audit system using Notion MCP","description":"I had built five audit skills for my Ikigai app - UX, Code Quality, Content Accuracy, Accessibility, and Security - where each one could analyze the app and generate unique, detailed findings as expert team personas. But the audit reports existed only as chat artifacts, and I wanted them in Notion where I could track fixes, link to GitHub issues, and measure progress over time.","date":"2025-11-08T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"notion-mcp-audits","title":"Watching Claude build an audit system using Notion MCP"},"unlisted":false,"prevItem":{"title":"Iterative auditing as a progress tracking mechanism","permalink":"/blog/analysis-paralysis"},"nextItem":{"title":"Building an app from a PDF (because AI can!)","permalink":"/blog/diving-into-ai"}},"content":"I had built five audit skills for my Ikigai app - UX, Code Quality, Content Accuracy, Accessibility, and Security - where each one could analyze the app and generate unique, detailed findings as expert team personas. But the audit reports existed only as chat artifacts, and I wanted them in Notion where I could track fixes, link to GitHub issues, and measure progress over time.\\n\\nOn prompting, Claude suggested the database schemas I\'d need in Notion to track audits: ten databases - five for audit sessions, five for individual findings - all with proper schemas and relations between them. \\n\\nI manually created one database in Notion to test the concept. Getting every property right - the select options, the number fields, the relations - took focus that would now need to last for ten databases.\\n\\nSo instead, I set up the Notion MCP connection and asked Claude to complete the task.\\n\x3c!--truncate --\x3e\\n\\n### The setup\\n\\nI followed <a href=\\"https://developers.notion.com/docs/mcp\\" target=\\"_blank\\">Notion\'s official MCP documentation</a>. After initial hiccups caused by Claude\'s integration docs, I created a Notion MCP -> Claude integration, got the API key, and added it to Claude Desktop\'s config file. The authentication part was straightforward.\\n\\nFair warning though, this integration needs refreshing everyday (or maybe that\'s just Claude being moody?); Claude refuses to sync with Notion when I try to pick up database tasks from where I left off the previous evening. I always rejig the config now before starting a database sync.\\n\\nNext, I directed Claude to write a Node.js setup script with checkpoint functionality in case anything failed midway. I needed it to create all ten databases with complete schemas, then link them together with relations. I can\'t, and didn\'t have to, write a single line of the setup code!\\n\\n### The moment!\\n\\nI ran the command, and Claude started creating the databases. I switched to Notion and hit Refresh. Ten databases appeared! Each one properly configured: UX Audit Sessions with all its metrics, UX Findings with severity levels and status tracking, Code Quality Sessions, Code Quality Findings, everything. \\n\\nIt warranted a little dance!\\n\\n### Running the first audit\\nThe app\'s first draft had obvious UX issues that jumped out at me immediately, and I wondered if the UX Auditor Skill would catch the same annoyances.\\n\\nI typed: \\n> Use the UX Auditor skill. File: xyz.html. After the audit, save results to Notion.\\n\\nTBH, I half-expected something to break, or have Claude tell me there\'s a property mismatch or a missing field, or that some piece of code wasn\'t working. \\n\\nClaude ran the audit and found 18 UX issues including one critical finding about the app being boring(!). The UX Auditor caught every issue that was bugging me at first glance, and then some. The audit was more nuanced than I expected, with detailed severity ratings and specific recommendations.\\n\\nClaude then searched for the relevant Notion databases, created an audit session entry, created 18 individual finding entries, linked them all together, and gave me the Notion URLs.\\n\\nJust like that.\\n\\n### The two-database pattern\\n\\nI hadn\'t immediately understood why Claude created two databases per audit type initially, but it made sense as soon as I looked at the audit data.\\n\\nFor each skill:\\n* One database holds the session, the macro view: date, file analyzed, total findings count, severity distribution, overall scores. The executive summary, if you will.\\n\\n* The other holds individual findings, the details. Each specific issue with its location, current state, recommendation, effort estimate, and status.\\n\\nSessions link to findings. Findings link back to sessions. You can view either the forest or the trees.\\n\\n### The (technical) reality check\\n\\nI ran the Content Accuracy Checker skill to see if I\u2019d just gotten lucky with the first audit or if this was a working idea. It was, to an extent, beginner\u2019s luck. While the audit ran great and Claude managed to publish everything to Notion, we hit a technical issue toward the end, and the chat timed out. My Claude chat history has no record of those Content Accuracy findings! \\n\\nThis made me think about all the previous instances where I\'ve lost entire chats because Claude timed out or hit context limits. Maybe writing Claude outputs to a linked external database should be the norm for any work?\\n\\n### What\'s next\\n\\nThe integration works well. I have two audits completed with reports in Notion, ready to track. I want to turn Notion findings into GitHub issues now to track completion and link to release notes, etc.\\n\\nThe audit skills themselves could become measurement tools. If I keep the check parameters unchanged, running audits at regular intervals could give me an objective progress marker. \\n```\\nFix issues --\x3e run audit again --\x3e compare results = hard quality numbers!\\n```\\n\\n### The learning\\nI now know to keep my requests to Claude simple. The clearer and more direct the task, the better the results. No need to overthink the prompts.\\n\\nThis wasn\'t about me writing clever code or complex prompts. It was about Claude understanding the schemas needed and building them correctly. The manual work I\'d done on that first database taught me what properties mattered. But scaling that to ten databases with relations, the setup script, the database schemas, the Notion integration, the audit execution was all Claude.\\n\\n(MCP FTW!)\\n\\nHere are snapshots of the Notion audits:\\n\\n![ux-audit](/img/ux-audit.png)\\n***\\n\\n![accessibility-audit](/img/accessibility-audit.png)\\n***\\n\\n![security-audit](/img/security-audit.png)\\n***\\n\\n![content audit](/img/content-audit.png)"},{"id":"diving-into-ai","metadata":{"permalink":"/blog/diving-into-ai","source":"@site/blog/2025-10-01-diving-into-ai.mdx","title":"Building an app from a PDF (because AI can!)","description":"AI and I built an interactive web app from a PDF, with drag-and-drop, auto-save, and an interactive five-step workflow, in a week! (I\'m thrilled about how well it works too!)","date":"2025-10-01T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"diving-into-ai","title":"Building an app from a PDF (because AI can!)"},"unlisted":false,"prevItem":{"title":"Watching Claude build an audit system using Notion MCP","permalink":"/blog/notion-mcp-audits"},"nextItem":{"title":"Sabbaticals - Yea or Nay?","permalink":"/blog/sabbaticals"}},"content":"AI and I built an interactive web app from a PDF, with drag-and-drop, auto-save, and an interactive five-step workflow, in a week! (I\'m thrilled about how well it works too!)\\n\\nThis project was about converting a PDF (and complex philosophical concepts) into intuitive user experiences while leveraging GitHub Copilot and Claude for rapid prototyping and problem-solving. \\n\\nI\'m documenting what I\'ve learned about collaborating with AI to build something real.\\n\x3c!-- truncate --\x3e\\n### Why Ikigai\\nI first learnt about \u201cIkigai\u201d in the book *\'Ikigai: The Japanese Secret to a Long and Happy Life\' by Francesc Miralles and Hector Garcia*. Although I later discovered that the book, as well as the Positive Psychology PDF that this app is is based on, are nothing like the actual Japanese concept; they\u2019re more a cultural appropriation of the eastern philosophy. \\n![Ikigai venn diagram](/img/ikigai.png)\\n\\nThat said, I find the framework insightful. The guided process of exploring passion, mission, vocation, and profession resonates with me, and I do think people could benefit from it as an interactive tool.\\n\\n### The fear\\nMy biggest concern isn\u2019t failure, it\'s not knowing how to handle AI-generated code if something goes wrong. If the code breaks or becomes messy, I wouldn\'t know where to begin fixing it, as a non-coder. I\'ve learned though, that the trick to working with AI is to build piece-by-piece, test frequently, and catch problems early.\\n\\n### First prompt\\nI started with:\\n\\n> ***I am building an Ikigai web-app based on the Ikigai PDF. Analyze the attached files and list the top improvements you would make to have a fully-functional, highly modern, well-designed interactive app that helps the reader find their Ikigai. \\n> <br/><br/>The PDF is the only source of information. \\n> Do not hallucinate new content about Ikigai, and do not create new artifacts unless asked to. \\n> Do not skim over or lose the details in the PDF. \\n> <br/><br/>Ask me questions on the intended behaviour, design, purpose, etc of the app until you have enough information for your research and analysis.***\\n\\n\\nThis prompt set the tone for everything. I established boundaries, provided constraints, and requested collaboration. It was a framework for how I wanted to work with AI.\\n\\n### What clicked\\nPrompt engineering felt abstract until I started practicing it. Prompts that clicked:\\n- **Few-shot prompts**: Showing examples helped Claude understand my expectations better than lengthy explanations.\\n- **Chain of thought prompts**: Asking Claude to think through problems step-by-step before generating code reduced errors.\\n- **Outline expansion prompts**: Starting with a simple structure and expanding section by section kept things manageable.\\n- **Flipped interaction prompts**: Instead of telling Claude what to do, I\'d ask it to interview me about my goals. This made the collaboration feel more like partnership.\\n\\n### What I like about the app\\n- Step 2: Guides users through questions about what they love, what they\'re good at, what the world needs, and what they can be paid for. The UI provides thoughtful prompts and self-reflective questions for each circle.\\n- Step 3: Find overlapping responses, where users identify which responses fit into multiple circles, and overlaps reveal patterns.\\n\\nThese steps feel deeply insightful. They\'re also the pages I plan to make more \u201cmine\u201d as the project evolves.\\n\\n### What\'s next\\nNow that Phase 1 is complete, here\'s the plan for Phase 2:\\n1. Run the audit skills\\n   \\n   Claude and I built 5 *Claude Skills* to audit the app:\\n   - UX/UI audit\\n   - Code quality review\\n   - Content accuracy check\\n   - Accessibility audit\\n   - Security analysis\\n\\n   I\u2019ve set up Notion to track the issues that these Skills generate. Claude designed the (ten) databases required to store these issues by Skill type. All I had to do was nudge it in the direction I wanted. (*More about this in the next blog!*)\\n\\n2. Fix issues\\n   - Set up an issue tracker via GitHub MCP with Notion\\n   - Find a way to automate issue fixes (ambitious, I know)\\n\\n### Frustrations and challenges\\nClaude routinely generated code for alternate files without me asking for them. Sometimes even after I specifically told it not to, it would create new artifacts when all I needed was iterations on what we had.\\n\\nI learned to use Claude Projects and project descriptions to set persistent context, and used a universal \'Truth Protocol\' to remind Claude of my working style. I even reset the chat a few times to get back on track. This wasn\'t Claude being bad; it was me learning how to communicate better.\\n\\nI couldn\'t get the Notion MCP working initially because of conflicting setup info from both Claude and Notion - the documentation didn\'t align and I hit a wall. I have a similar challenge ahead: integrating with GitHub MCP to make app updates trackable through versions.\\n\\nI\'m also about to run audit skills on the app for UX, accessibility, security, code quality, and content accuracy, and the resulting number of issues might get overwhelming.\\n\\n### What I\'ve learned\\nI\'m excited at having gotten this far! With AI, I just have to think an idea through and choose my prompts.\\nMy top takeaways:\\n1. AI slop is real. A human in the loop is non-negotiable. AI is a tool, not the solution.\\n2. Start with constraints. Tell the AI what not to do as clearly as what to do.\\n3. Collaborate, don\'t dictate. Ask AI to interview you about your goals before generating solutions.\\n4. Test frequently. The worst-case scenario isn\'t building something broken. It\'s building something you can\'t debug.\\n5. Use Projects to set persistent context. It saves you from repeating yourself.\\n6. Expect frustration; AI will misunderstand you, and you\'ll reset chats. That\'s part of the process!\\n7. Training in prompt engineering helped me a lot. I highly recommend the entire series by Prof Jules White, Vanderbilt University on Coursera.\\n\\n### Why this matters\\nI enjoy research and problem solving. I haven\'t pinpointed my mission or even my *ikigai* yet, but I do enjoy the learning that came with building this app.\\nAnd maybe that\'s just it!"},{"id":"sabbaticals","metadata":{"permalink":"/blog/sabbaticals","source":"@site/blog/2020-12-30-sabbaticals.mdx","title":"Sabbaticals - Yea or Nay?","description":"YEA!","date":"2020-12-30T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"sabbaticals","title":"Sabbaticals - Yea or Nay?"},"unlisted":false,"prevItem":{"title":"Building an app from a PDF (because AI can!)","permalink":"/blog/diving-into-ai"},"nextItem":{"title":"Design Thinking for solo technical writers","permalink":"/blog/design-thinking"}},"content":"**YEA!**\\n\\nAfter 12 years of tech writing in the corporate world, I went on a sabbatical in 2016. I had no plan or alternate source of income. (Must say, a little too adventurous even for me!) \\n\x3c!-- truncate --\x3e\\nI was driven to do something, *anything*, radically different from my routine.\\n\\nIn the months that followed, I immersed myself in online courses ranging from UX to entrepreneurship. Whatever caught my fancy, really. Spent my free time volunteering at <a href=\\"https://charlies-care.com/\\" target=\\"/_blank\\">my favorite animal shelter</a>, traveling, and clicking pics of <a href=\\"https://www.flickr.com/photos/sabitarao/48670537817/\\" target=\\"_blank\\">my muse</a>. I completed my AOW scuba certification too, and seriously contemplated a career change!  \\n\\n![scuba diving](/img/scuba.jpeg)\\n*(Clicked at 30m/100ft on the Great Barrier Reef seafloor. I\'m in the middle, with the funny hair.)*  \\n\\nIn early 2018, I dived into courses again, this time with a purpose. I focused on <a href=\\"https://www.freecodecamp.org/sabitarao\\" target=\\"_blank\\">front-end web development</a> and Docs as Code. Started giving corporate interviews to see where I stood in the tech writing world that I\'d left behind.\\n\\nInterviews can whip you into shape like nothing else! The feedback and advice I received from interviewers helped me dig deep, acknowledge my weaknesses, and flaunt my strengths. \\n\\n2019 saw me apply to Google\'s Season of Docs and not make the cut. That was rough, but helped me realize the caliber of writers I was up against. So I improved my skills, participated in Hacktoberfest, and landed a role in a startup. For a geek like me, being immersed in a barrage of new tech, surrounded by brilliant minds, and practically getting paid to learn, is *gold*. \\n\\nNone of this (okay, maybe *some*) would\'ve happened without a sabbatical. It made me *want* to be a technical writer again. That little step away packed a ton of perspective!  \\n\\n(As for scuba diving, well, it\'s not off the table. :-D ) \\n\\nIf a clean break from work isn\'t your thing, that\'s fine too. Find other ways to infuse your routine with new energy. Volunteer, plan weekend getaways, learn a new skill at work, participate in hackathons, the list is endless. The keyword here is *balance*. Don\'t bite off more than you can chew, and don\'t compare yourself to others.\\n\\nP.S. I ended 2020 as a contributor to CERN-DESY in Google\'s Season of Docs! :) More <a href = \\"https://github.com/sabitarao/gsod/wiki\\">here</a>."},{"id":"design-thinking","metadata":{"permalink":"/blog/design-thinking","source":"@site/blog/2020-04-11-design-thinking.mdx","title":"Design Thinking for solo technical writers","description":"What does design thinking have to do with writing, or any task, really?","date":"2020-04-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"design-thinking","title":"Design Thinking for solo technical writers"},"unlisted":false,"prevItem":{"title":"Sabbaticals - Yea or Nay?","permalink":"/blog/sabbaticals"}},"content":"What does design thinking have to do with writing, or any task, really?\\n\x3c!-- truncate --\x3e\\n<p>![design thinking work flow](/img/design-thinking.jpg) Image credits: <a href=\\"https://www.flickr.com/photos/christineprefontaine/8667743577\\" target=\\"_blank\\">Christine Prefontaine</a></p>   \\n\\nI\'m an IBM-certified <a href=\\"https://www.credly.com/badges/573fca7e-9b8e-4a7b-a50e-13088787002f/public_url\\" target=\\"_blank\\">Design Thinking Co-Creator</a>. It\'s serendipity! I stumbled upon IBM\'s <a href=\\"https://www.ibm.com/design/thinking/page/courses/Practitioner\\" target=\\"_blank\\">series</a> a year ago, while googling unrelated topics.  \\n\\nAs a technical writer, it\'s my job to focus on user goals in all my tasks. And as a remote worker, I sometimes run the risk of missing outliers while planning doc structures, followed by writing sprints, reviews, improvs...you know the drill.  \\n\\nIt\'s hard then, to not dread stakeholders\' meetings that could result in drastic edits, threatening those perfectly-crafted sentences and info architecture layouts.\\n\\nAnd that\'s exactly the sort of risk that design thinking can *minimize*. With a specific set of tools and processes, it requires team members across job roles to truly **collaborate**, keeping end-user goals at the center of discussions. Everyone in the team stands to gain.  \\nFor example, your perspective on the product UI or workflows can improve the end user\'s experience. Similarly, giving dev teams a say in doc design/planning/publishing makes them care about product docs. True story!  \\n\\n![restless reinvention](/img/restless-reinvention.PNG)  \\nDesign thinking reminds me to focus on empathy, human-centered solutions, and continuous improvement.  \\nIf I have to pick one aspect of design thinking that (sole) writers must imbibe, it\'s this:  \\n*Fail fast and cheap*.  \\nBy *not* aiming for a \'perfect\' draft, I spend less time theorizing about possible solutions, and more time actually improving ones that work.  \\n\\n>**The only constant in life is change.** \\n>~Heraclitus"}]}}')}}]);